{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Partition SBM Baseline Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Doc string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "#\n",
    "# Copyright 2017 MIT Lincoln Laboratory, Massachusetts Institute of Technology\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use these files except in compliance with\n",
    "# the License.\n",
    "#\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n",
    "# an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n",
    "# specific language governing permissions and limitations under the License.\n",
    "#\n",
    "\n",
    "\"\"\"\n",
    "Authors: Steven Smith, Edward Kao\n",
    "Date: 9 January 2017\n",
    "Installation: Python 2.7\n",
    "\n",
    "Description: This Python script performs the baseline graph partition algorithm based on the degree-corrected stochastic block model.\n",
    "\n",
    "References:\n",
    "Peixoto, Tiago P. \"Entropy of stochastic blockmodel ensembles.\" Physical Review E 85, no. 5 (2012): 056122.\n",
    "Peixoto, Tiago P. \"Parsimonious module inference in large networks.\" Physical review letters 110, no. 14 (2013): 148701.\n",
    "Karrer, Brian, and Mark EJ Newman. \"Stochastic blockmodels and community structure in networks.\" Physical Review E 83, no. 1 (2011): 016107.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd # for loading input graph TSV files\n",
    "import numpy as np\n",
    "from scipy import sparse as sparse\n",
    "import scipy.misc as misc\n",
    "from munkres import Munkres # for correctness evaluation\n",
    "\n",
    "use_timeit = True # for timing runs (optional)\n",
    "if use_timeit:\n",
    "    import timeit \n",
    "\n",
    "use_graph_tool_options = True # for visualiziing graph partitions (optional)\n",
    "if use_graph_tool_options:\n",
    "    import graph_tool.all as gt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Graph data structure\n",
    "\n",
    "A graph is represented as a list of numpy arrays. Each element in the outer array represents the array of vertex neighbors, and each row in the inner arrays has three values representing a directed edge betwee the neighbors (from, to) with edge weight (default to 1). \n",
    "\n",
    "A vertex label hash is stored from the input triple store file format. Vertices are internally indexed by an integer value from 0 to *N*&ndash;1, where *N* is the number of vertices.\n",
    "\n",
    "The truth and output partitions are stored as numpy arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Supporting functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load the graph from standard TSV files, and the truth partition if available\n",
    "\n",
    "The tsv file is assumed to be of the form \"from to [weight]\" (tab delimited). If available, the true partition is assumed to be stored in the file `filename_truePartition.tsv`. Nodes are assumed to be indexed from *0* to *N-1* in the input file.\n",
    "### Parameters\n",
    "    input_filename : str\n",
    "            input file name not including the .tsv extension\n",
    "    true_partition_available : bool\n",
    "            whether the truth partition is available\n",
    "### Returns\n",
    "    out_neighbors : list of ndarray; list length is N, the number of nodes\n",
    "            each element of the list is a ndarray of out neighbors, where the first column is the node indices\n",
    "            and the second column the corresponding edge weights\n",
    "    in_neighbors : list of ndarray; list length is N, the number of nodes\n",
    "            each element of the list is a ndarray of in neighbors, where the first column is the node indices\n",
    "            and the second column the corresponding edge weights\n",
    "    N : int\n",
    "            number of nodes in the graph\n",
    "    E : int\n",
    "            number of edges in the graph\n",
    "    true_b : ndarray (int) optional\n",
    "            array of truth block assignment for each node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def load_graph(input_filename, true_partition_available):\n",
    "    # read the entire graph CSV into rows of edges\n",
    "    edge_rows = pd.read_csv('{}.tsv'.format(input_filename), delimiter='\\t', header=None).as_matrix()\n",
    "    N = edge_rows[:, 0:1].max()\n",
    "    out_neighbors = [[] for i in range(N)]\n",
    "    in_neighbors = [[] for i in range(N)]\n",
    "    weights_included = edge_rows.shape[1] == 3\n",
    "\n",
    "    # load edges to list of lists of out and in neighbors\n",
    "    for i in range(edge_rows.shape[0]):\n",
    "        if weights_included:\n",
    "            edge_weight = edge_rows[i, 2]\n",
    "        else:\n",
    "            edge_weight = 1\n",
    "        # -1 on the node index since Python is 0-indexed and the standard graph TSV is 1-indexed\n",
    "        out_neighbors[edge_rows[i, 0]-1].append([edge_rows[i, 1]-1, edge_weight])\n",
    "        in_neighbors[edge_rows[i, 1]-1].append([edge_rows[i, 0]-1, edge_weight])\n",
    "\n",
    "    # convert each neighbor list to neighbor numpy arrays for faster access\n",
    "    for i in range(N):\n",
    "        out_neighbors[i] = np.array(out_neighbors[i], dtype=int)\n",
    "    for i in range(N):\n",
    "        in_neighbors[i] = np.array(in_neighbors[i], dtype=int)\n",
    "\n",
    "    # find number of nodes and edges\n",
    "    N = len(out_neighbors)\n",
    "    E = sum(len(v) for v in out_neighbors)\n",
    "\n",
    "    if true_partition_available:\n",
    "        true_b = np.zeros(len(out_neighbors), dtype=int)\n",
    "        # read the entire true partition CSV into rows of partitions\n",
    "        true_b_rows = pd.read_csv('{}_truePartition.tsv'.format(input_filename), delimiter='\\t', header=None).as_matrix()\n",
    "        for i in range(true_b_rows.shape[0]):\n",
    "            true_b[true_b_rows[i, 0]-1] = int(true_b_rows[i, 1]-1) # -1 since Python is 0-indexed and the TSV is 1-indexed\n",
    "\n",
    "    if true_partition_available:\n",
    "        return out_neighbors, in_neighbors, N, E, true_b\n",
    "    else:\n",
    "        return out_neighbors, in_neighbors, N, E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Initialize variables for the partition iterations\n",
    "This function returns initialized variables for the iterations to find the best partition with the optimal number of blocks\n",
    "\n",
    "### Returns\n",
    "    optimal_B_found : bool\n",
    "            flag for whether the optimal block has been found\n",
    "    old_b : list of length 3\n",
    "            holds the best three partitions so far\n",
    "    old_M : list of length 3\n",
    "            holds the edge count matrices for the best three partitions so far\n",
    "    old_d : list of length 3\n",
    "            holds the block degrees for the best three partitions so far\n",
    "    old_d_out : list of length 3\n",
    "            holds the out block degrees for the best three partitions so far\n",
    "    old_d_in : list of length 3\n",
    "            holds the in block degrees for the best three partitions so far\n",
    "    old_S : list of length 3\n",
    "            holds the overall entropy for the best three partitions so far\n",
    "    old_B : list of length 3\n",
    "            holds the number of blocks for the best three partitions so far\n",
    "    graph_object : list\n",
    "            empty for now and will store the graph object if graphs will be visualized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def initialize_partition_variables():\n",
    "    optimal_B_found = False\n",
    "    old_b = [[], [], []]  # partition for the high, best, and low number of blocks so far\n",
    "    old_M = [[], [], []]  # edge count matrix for the high, best, and low number of blocks so far\n",
    "    old_d = [[], [], []]  # block degrees for the high, best, and low number of blocks so far\n",
    "    old_d_out = [[], [], []]  # out block degrees for the high, best, and low number of blocks so far\n",
    "    old_d_in = [[], [], []]  # in block degrees for the high, best, and low number of blocks so far\n",
    "    old_S = [np.Inf, np.Inf, np.Inf] # overall entropy for the high, best, and low number of blocks so far\n",
    "    old_B = [[], [], []]  # number of blocks for the high, best, and low number of blocks so far\n",
    "    graph_object = []\n",
    "    return optimal_B_found, old_b, old_M, old_d, old_d_out, old_d_in, old_S, old_B, graph_object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Initialize the edge count matrix and block degrees according to the current partition\n",
    "\n",
    "### Parameters\n",
    "    out_neighbors : list of ndarray; list length is N, the number of nodes\n",
    "            each element of the list is a ndarray of out neighbors, where the first column is the node indices\n",
    "            and the second column the corresponding edge weights\n",
    "    B : int\n",
    "            total number of blocks in the current partition\n",
    "    b : ndarray (int)\n",
    "            array of block assignment for each node\n",
    "    use_sparse : bool\n",
    "            whether the edge count matrix is stored as a sparse matrix\n",
    "### Returns\n",
    "    M : ndarray or sparse matrix (int), shape = (#blocks, #blocks)\n",
    "            edge count matrix between all the blocks.\n",
    "    d_out : ndarray (int)\n",
    "            the current out degree of each block\n",
    "    d_in : ndarray (int)\n",
    "            the current in degree of each block\n",
    "    d : ndarray (int)\n",
    "            the current total degree of each block\n",
    "### Notes\n",
    "Compute the edge count matrix and the block degrees from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def initialize_edge_counts(out_neighbors, B, b, use_sparse):\n",
    "    if use_sparse: # store interblock edge counts as a sparse matrix\n",
    "        M = sparse.lil_matrix((B, B), dtype=int)\n",
    "    else:\n",
    "        M = np.zeros((B, B), dtype=int)\n",
    "    # compute the initial interblock edge count\n",
    "    for v in range(len(out_neighbors)):\n",
    "        k1 = b[v]\n",
    "        k2, inverse_idx = np.unique(b[out_neighbors[v][:, 0]], return_inverse=True)\n",
    "        count = np.bincount(inverse_idx, weights=out_neighbors[v][:, 1]).astype(int)\n",
    "        M[k1, k2] += count\n",
    "    # compute initial block degrees\n",
    "    d_out = np.asarray(M.sum(axis=1)).ravel()\n",
    "    d_in = np.asarray(M.sum(axis=0)).ravel()\n",
    "    d = d_out + d_in\n",
    "    return M, d_out, d_in, d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Propose a new block assignment for the current node or block\n",
    "\n",
    "### Parameters\n",
    "    r : int\n",
    "            current block assignment for the node under consideration\n",
    "    neighbors_out : ndarray (int) of two columns\n",
    "            out neighbors array where the first column is the node indices and the second column is the edge weight\n",
    "    neighbors_in : ndarray (int) of two columns\n",
    "            in neighbors array where the first column is the node indices and the second column is the edge weight\n",
    "    b : ndarray (int)\n",
    "            array of block assignment for each node\n",
    "    M : ndarray or sparse matrix (int), shape = (#blocks, #blocks)\n",
    "            edge count matrix between all the blocks.\n",
    "    d : ndarray (int)\n",
    "            total number of edges to and from each block\n",
    "    B : int\n",
    "            total number of blocks\n",
    "    agg_move : bool\n",
    "            whether the proposal is a block move\n",
    "    use_sparse : bool\n",
    "            whether the edge count matrix is stored as a sparse matrix\n",
    "\n",
    "### Returns\n",
    "    s : int\n",
    "            proposed block assignment for the node under consideration\n",
    "    k_out : int\n",
    "            the out degree of the node\n",
    "    k_in : int\n",
    "            the in degree of the node\n",
    "    k : int\n",
    "            the total degree of the node\n",
    "\n",
    "### Notes\n",
    "- $d_u$: degree of block u\n",
    "\n",
    "Randomly select a neighbor of the current node, and obtain its block assignment $u$. With probability $\\frac{B}{d_u + B}$, randomly propose\n",
    "a block. Otherwise, randomly selects a neighbor to block $u$ and propose its block assignment. For block (agglomerative) moves,\n",
    "avoid proposing the current block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def propose_new_partition(r, neighbors_out, neighbors_in, b, M, d, B, agg_move, use_sparse):\n",
    "    neighbors = np.concatenate((neighbors_out, neighbors_in))\n",
    "    k_out = sum(neighbors_out[:,1])\n",
    "    k_in = sum(neighbors_in[:,1])\n",
    "    k = k_out + k_in\n",
    "    rand_neighbor = np.random.choice(neighbors[:,0], p=neighbors[:,1]/float(k))\n",
    "    u = b[rand_neighbor]\n",
    "    # propose a new block randomly\n",
    "    if np.random.uniform() <= B/float(d[u]+B):  # chance inversely prop. to block_degree\n",
    "        if agg_move:  # force proposal to be different from current block\n",
    "            candidates = set(range(B))\n",
    "            candidates.discard(r)\n",
    "            s = np.random.choice(list(candidates))\n",
    "        else:\n",
    "            s = np.random.randint(B)\n",
    "    else:  # propose by random draw from neighbors of block partition[rand_neighbor]\n",
    "        if use_sparse:\n",
    "            multinomial_prob = (M[u, :].toarray().transpose() + M[:, u].toarray()) / float(d[u])\n",
    "        else:\n",
    "            multinomial_prob = (M[u, :].transpose() + M[:, u]) / float(d[u])\n",
    "        if agg_move:  # force proposal to be different from current block\n",
    "            multinomial_prob[r] = 0\n",
    "            if multinomial_prob.sum() == 0:  # the current block has no neighbors. randomly propose a different block\n",
    "                candidates = set(range(B))\n",
    "                candidates.discard(r)\n",
    "                s = np.random.choice(list(candidates))\n",
    "                return s, k_out, k_in, k\n",
    "            else:\n",
    "                multinomial_prob = multinomial_prob / multinomial_prob.sum()\n",
    "        candidates = multinomial_prob.nonzero()[0]\n",
    "        s = candidates[np.flatnonzero(np.random.multinomial(1, multinomial_prob[candidates].ravel()))[0]]\n",
    "    return s, k_out, k_in, k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Compute the two new rows and cols of the edge count matrix under the proposal for the current node or block\n",
    "\n",
    "### Parameters\n",
    "    M : ndarray or sparse matrix (int), shape = (#blocks, #blocks)\n",
    "            edge count matrix between all the blocks.\n",
    "    r : int\n",
    "            current block assignment for the node under consideration\n",
    "    s : int\n",
    "            proposed block assignment for the node under consideration\n",
    "    b_out : ndarray (int)\n",
    "            blocks of the out neighbors\n",
    "    count_out : ndarray (int)\n",
    "            edge counts to the out neighbor blocks\n",
    "    b_in : ndarray (int)\n",
    "            blocks of the in neighbors\n",
    "    count_in : ndarray (int)\n",
    "            edge counts to the in neighbor blocks\n",
    "    count_self : int\n",
    "            edge counts to self\n",
    "    agg_move : bool\n",
    "            whether the proposal is a block move\n",
    "    use_sparse : bool\n",
    "            whether the edge count matrix is stored as a sparse matrix\n",
    "\n",
    "### Returns\n",
    "    M_r_row : ndarray or sparse matrix (int)\n",
    "            the current block row of the new edge count matrix under proposal\n",
    "    M_s_row : ndarray or sparse matrix (int)\n",
    "            the proposed block row of the new edge count matrix under proposal\n",
    "    M_r_col : ndarray or sparse matrix (int)\n",
    "            the current block col of the new edge count matrix under proposal\n",
    "    M_s_col : ndarray or sparse matrix (int)\n",
    "            the proposed block col of the new edge count matrix under proposal\n",
    "\n",
    "### Notes\n",
    "The updates only involve changing the entries to and from the neighboring blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def compute_new_rows_cols_interblock_edge_count_matrix(M, r, s, b_out, count_out, b_in, count_in, count_self, agg_move, use_sparse):\n",
    "    B = M.shape[0]\n",
    "    if agg_move:  # the r row and column are simply empty after this merge move\n",
    "        if use_sparse:\n",
    "            M_r_row = sparse.lil_matrix(M[r, :].shape, dtype=int)\n",
    "            M_r_col = sparse.lil_matrix(M[:, r].shape, dtype=int)\n",
    "        else:\n",
    "            M_r_row = np.zeros((1, B), dtype=int)\n",
    "            M_r_col = np.zeros((B, 1), dtype=int)\n",
    "    else:\n",
    "        if use_sparse:\n",
    "            M_r_row = M[r, :].copy()\n",
    "            M_r_col = M[:, r].copy()\n",
    "        else:\n",
    "            M_r_row = M[r, :].copy().reshape(1, B)\n",
    "            M_r_col = M[:, r].copy().reshape(B, 1)\n",
    "        M_r_row[0, b_out] -= count_out\n",
    "        M_r_row[0, r] -= np.sum(count_in[np.where(b_in == r)])\n",
    "        M_r_row[0, s] += np.sum(count_in[np.where(b_in == r)])\n",
    "        M_r_col[b_in, 0] -= count_in.reshape(M_r_col[b_in, 0].shape)\n",
    "        M_r_col[r, 0] -= np.sum(count_out[np.where(b_out == r)])\n",
    "        M_r_col[s, 0] += np.sum(count_out[np.where(b_out == r)])\n",
    "    if use_sparse:\n",
    "        M_s_row = M[s, :].copy()\n",
    "        M_s_col = M[:, s].copy()\n",
    "    else:\n",
    "        M_s_row = M[s, :].copy().reshape(1, B)\n",
    "        M_s_col = M[:, s].copy().reshape(B, 1)\n",
    "    M_s_row[0, b_out] += count_out\n",
    "    M_s_row[0, r] -= np.sum(count_in[np.where(b_in == s)])\n",
    "    M_s_row[0, s] += np.sum(count_in[np.where(b_in == s)])\n",
    "    M_s_row[0, r] -= count_self\n",
    "    M_s_row[0, s] += count_self\n",
    "    M_s_col[b_in, 0] += count_in.reshape(M_s_col[b_in, 0].shape)\n",
    "    M_s_col[r, 0] -= np.sum(count_out[np.where(b_out == s)])\n",
    "    M_s_col[s, 0] += np.sum(count_out[np.where(b_out == s)])\n",
    "    M_s_col[r, 0] -= count_self\n",
    "    M_s_col[s, 0] += count_self\n",
    "    return M_r_row, M_s_row, M_r_col, M_s_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Compute the new block degrees under the proposal for the current node or block\n",
    "\n",
    "### Parameters\n",
    "    r : int\n",
    "            current block assignment for the node under consideration\n",
    "    s : int\n",
    "            proposed block assignment for the node under consideration\n",
    "    d_out : ndarray (int)\n",
    "            the current out degree of each block\n",
    "    d_in : ndarray (int)\n",
    "            the current in degree of each block\n",
    "    d : ndarray (int)\n",
    "            the current total degree of each block\n",
    "    k_out : int\n",
    "            the out degree of the node\n",
    "    k_in : int\n",
    "            the in degree of the node\n",
    "    k : int\n",
    "            the total degree of the node\n",
    "\n",
    "### Returns\n",
    "    d_out_new : ndarray (int)\n",
    "            the new out degree of each block under proposal\n",
    "    d_in_new : ndarray (int)\n",
    "            the new in degree of each block under proposal\n",
    "    d_new : ndarray (int)\n",
    "            the new total degree of each block under proposal\n",
    "\n",
    "### Notes\n",
    "The updates only involve changing the degrees of the current and proposed block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def compute_new_block_degrees(r, s, d_out, d_in, d, k_out, k_in, k):\n",
    "    new = []\n",
    "    for old, degree in zip([d_out, d_in, d], [k_out, k_in, k]):\n",
    "        new_d = old.copy()\n",
    "        new_d[r] -= degree\n",
    "        new_d[s] += degree\n",
    "        new.append(new_d)\n",
    "    return new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Compute the Hastings correction for the proposed block from the current block\n",
    "\n",
    "### Parameters\n",
    "    b_out : ndarray (int)\n",
    "            blocks of the out neighbors\n",
    "    count_out : ndarray (int)\n",
    "            edge counts to the out neighbor blocks\n",
    "    b_in : ndarray (int)\n",
    "            blocks of the in neighbors\n",
    "    count_in : ndarray (int)\n",
    "            edge counts to the in neighbor blocks\n",
    "    s : int\n",
    "            proposed block assignment for the node under consideration\n",
    "    M : ndarray or sparse matrix (int), shape = (#blocks, #blocks)\n",
    "            edge count matrix between all the blocks.\n",
    "    M_r_row : ndarray or sparse matrix (int)\n",
    "            the current block row of the new edge count matrix under proposal\n",
    "    M_r_col : ndarray or sparse matrix (int)\n",
    "            the current block col of the new edge count matrix under proposal\n",
    "    B : int\n",
    "            total number of blocks\n",
    "    d : ndarray (int)\n",
    "            total number of edges to and from each block\n",
    "    d_new : ndarray (int)\n",
    "            new block degrees under the proposal\n",
    "    use_sparse : bool\n",
    "            whether the edge count matrix is stored as a sparse matrix\n",
    "\n",
    "### Returns\n",
    "    Hastings_correction : float\n",
    "            term that corrects for the transition asymmetry between the current block and the proposed block\n",
    "### Notes\n",
    "- $p_{i, s \\rightarrow r}$ : for node $i$, probability of proposing block $r$ if its current block is $s$\n",
    "- $p_{i, r \\rightarrow s}$ : for node $i$, probability of proposing block $s$ if its current block is $r$\n",
    "- $r$ : current block for node $i$\n",
    "- $s$ : proposed block for node $i$\n",
    "- $M^-$: current edge count matrix between the blocks\n",
    "- $M^+$: new edge count matrix under the proposal\n",
    "- $d^-_t$: current degree of block $t$\n",
    "- $d^+_t$: new degree of block $t$ under the proposal\n",
    "- $\\mathbf{b}_{\\mathcal{N}_i}$: the neighboring blocks to node $i$\n",
    "- $k_i$: the degree of node $i$\n",
    "- $k_{i,t}$ : the degree of node $i$ to block $t$ (i.e. number of edges to and from block $t$)\n",
    "- $B$ : the number of blocks\n",
    "\n",
    "The Hastings correction is: \n",
    "\n",
    "$\\huge \\frac{p_{i, s \\rightarrow r}}{p_{i, r \\rightarrow s}}$\n",
    "\n",
    "where\n",
    "\n",
    "$\\Large p_{i, r \\rightarrow s} = \\sum_{t \\in \\{\\mathbf{b}_{\\mathcal{N}_i}^-\\}} \\left[ {\\frac{k_{i,t}}{k_i} \\frac{M_{ts}^- + M_{st}^- + 1}{d^-_t+B}}\\right]$\n",
    "\n",
    "$\\Large p_{i, s \\rightarrow r} = \\sum_{t \\in \\{\\mathbf{b}_{\\mathcal{N}_i}^-\\}} \\left[ {\\frac{k_{i,t}}{k_i} \\frac{M_{tr}^+ + M_{rt}^+ +1}{d_t^++B}}\\right]$\n",
    "\n",
    "summed over all the neighboring blocks $t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def compute_Hastings_correction(b_out, count_out, b_in, count_in, s, M, M_r_row, M_r_col, B, d, d_new, use_sparse):\n",
    "    t, idx = np.unique(np.append(b_out, b_in), return_inverse=True)  # find all the neighboring blocks\n",
    "    count = np.bincount(idx, weights=np.append(count_out, count_in)).astype(int)  # count edges to neighboring blocks\n",
    "    if use_sparse:\n",
    "        M_t_s = M[t, s].toarray().ravel()\n",
    "        M_s_t = M[s, t].toarray().ravel()\n",
    "        M_r_row = M_r_row[0, t].toarray().ravel()\n",
    "        M_r_col = M_r_col[t, 0].toarray().ravel()\n",
    "    else:\n",
    "        M_t_s = M[t, s].ravel()\n",
    "        M_s_t = M[s, t].ravel()\n",
    "        M_r_row = M_r_row[0, t].ravel()\n",
    "        M_r_col = M_r_col[t, 0].ravel()\n",
    "        \n",
    "    p_forward = np.sum(count*(M_t_s + M_s_t + 1) / (d[t] + float(B)))\n",
    "    p_backward = np.sum(count*(M_r_row + M_r_col + 1) / (d_new[t] + float(B)))\n",
    "    return p_backward / p_forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Compute change in entropy under the proposal\n",
    "Reduction in entropy means the proposed block is better than the current block.\n",
    "\n",
    "### Parameters\n",
    "    r : int\n",
    "            current block assignment for the node under consideration\n",
    "    s : int\n",
    "            proposed block assignment for the node under consideration\n",
    "    M : ndarray or sparse matrix (int), shape = (#blocks, #blocks)\n",
    "            edge count matrix between all the blocks.\n",
    "    M_r_row : ndarray or sparse matrix (int)\n",
    "            the current block row of the new edge count matrix under proposal\n",
    "    M_s_row : ndarray or sparse matrix (int)\n",
    "            the proposed block row of the new edge count matrix under proposal\n",
    "    M_r_col : ndarray or sparse matrix (int)\n",
    "            the current block col of the new edge count matrix under proposal\n",
    "    M_s_col : ndarray or sparse matrix (int)\n",
    "            the proposed block col of the new edge count matrix under proposal\n",
    "    d_out : ndarray (int)\n",
    "            the current out degree of each block\n",
    "    d_in : ndarray (int)\n",
    "            the current in degree of each block\n",
    "    d_out_new : ndarray (int)\n",
    "            the new out degree of each block under proposal\n",
    "    d_in_new : ndarray (int)\n",
    "            the new in degree of each block under proposal\n",
    "    use_sparse : bool\n",
    "            whether the edge count matrix is stored as a sparse matrix\n",
    "\n",
    "### Returns\n",
    "    delta_entropy : float\n",
    "            entropy under the proposal minus the current entropy\n",
    "\n",
    "### Notes\n",
    "- $M^-$: current edge count matrix between the blocks\n",
    "- $M^+$: new edge count matrix under the proposal\n",
    "- $d^-_{t, \\rm in}$: current in degree of block $t$\n",
    "- $d^-_{t, \\rm out}$: current out degree of block $t$\n",
    "- $d^+_{t, \\rm in}$: new in degree of block $t$ under the proposal\n",
    "- $d^+_{t, \\rm out}$: new out degree of block $t$ under the proposal\n",
    "\n",
    "The difference in entropy is computed as:\n",
    "\n",
    "$\\large \\Delta S = \\sum_{t_1, t_2} {\\left[ -M_{t_1 t_2}^+ \\log\\left(\\frac{M_{t_1 t_2}^+}{d_{t_1, \\rm out}^+ d_{t_2, \\rm in}^+}\\right) + M_{t_1 t_2}^- \\log\\left(\\frac{M_{t_1 t_2}^-}{d_{t_1, \\rm out}^- d_{t_2, \\rm in}^-}\\right)\\right]}$\n",
    "\n",
    "where the sum runs over all entries $(t_1, t_2)$ in rows and cols $r$ and $s$ of the edge count matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def compute_delta_entropy(r, s, M, M_r_row, M_s_row, M_r_col, M_s_col, d_out, d_in, d_out_new, d_in_new, use_sparse):\n",
    "    if use_sparse: # computation in the sparse matrix is slow so convert to numpy arrays since operations are on only two rows and cols\n",
    "        M_r_row = M_r_row.toarray()\n",
    "        M_s_row = M_s_row.toarray()\n",
    "        M_r_col = M_r_col.toarray()\n",
    "        M_s_col = M_s_col.toarray()\n",
    "        M_r_t1 = M[r, :].toarray()\n",
    "        M_s_t1 = M[s, :].toarray()\n",
    "        M_t2_r = M[:, r].toarray()\n",
    "        M_t2_s = M[:, s].toarray()\n",
    "    else:\n",
    "        M_r_t1 = M[r, :]\n",
    "        M_s_t1 = M[s, :]\n",
    "        M_t2_r = M[:, r]\n",
    "        M_t2_s = M[:, s]\n",
    "\n",
    "    # remove r and s from the cols to avoid double counting\n",
    "    idx = range(len(d_in_new))\n",
    "    del idx[max(r,s)]\n",
    "    del idx[min(r,s)]\n",
    "    M_r_col = M_r_col[idx]\n",
    "    M_s_col = M_s_col[idx]\n",
    "    M_t2_r = M_t2_r[idx]\n",
    "    M_t2_s = M_t2_s[idx]\n",
    "    d_out_new_ = d_out_new[idx]\n",
    "    d_out_ = d_out[idx]\n",
    "\n",
    "    # only keep non-zero entries to avoid unnecessary computation\n",
    "    d_in_new_r_row = d_in_new[M_r_row.ravel().nonzero()]\n",
    "    d_in_new_s_row = d_in_new[M_s_row.ravel().nonzero()]\n",
    "    M_r_row = M_r_row[M_r_row.nonzero()]\n",
    "    M_s_row = M_s_row[M_s_row.nonzero()]\n",
    "    d_out_new_r_col = d_out_new_[M_r_col.ravel().nonzero()]\n",
    "    d_out_new_s_col = d_out_new_[M_s_col.ravel().nonzero()]\n",
    "    M_r_col = M_r_col[M_r_col.nonzero()]\n",
    "    M_s_col = M_s_col[M_s_col.nonzero()]\n",
    "    d_in_r_t1 = d_in[M_r_t1.ravel().nonzero()]\n",
    "    d_in_s_t1 = d_in[M_s_t1.ravel().nonzero()]\n",
    "    M_r_t1= M_r_t1[M_r_t1.nonzero()]\n",
    "    M_s_t1 = M_s_t1[M_s_t1.nonzero()]\n",
    "    d_out_r_col = d_out_[M_t2_r.ravel().nonzero()]\n",
    "    d_out_s_col = d_out_[M_t2_s.ravel().nonzero()]\n",
    "    M_t2_r = M_t2_r[M_t2_r.nonzero()]\n",
    "    M_t2_s = M_t2_s[M_t2_s.nonzero()]\n",
    "\n",
    "    # sum over the two changed rows and cols\n",
    "    delta_entropy = 0\n",
    "    delta_entropy -= np.sum(M_r_row * np.log(M_r_row.astype(float) / d_in_new_r_row / d_out_new[r]))\n",
    "    delta_entropy -= np.sum(M_s_row * np.log(M_s_row.astype(float) / d_in_new_s_row / d_out_new[s]))\n",
    "    delta_entropy -= np.sum(M_r_col * np.log(M_r_col.astype(float) / d_out_new_r_col / d_in_new[r]))\n",
    "    delta_entropy -= np.sum(M_s_col * np.log(M_s_col.astype(float) / d_out_new_s_col / d_in_new[s]))\n",
    "    delta_entropy += np.sum(M_r_t1 * np.log(M_r_t1.astype(float) / d_in_r_t1 / d_out[r]))\n",
    "    delta_entropy += np.sum(M_s_t1 * np.log(M_s_t1.astype(float) / d_in_s_t1 / d_out[s]))\n",
    "    delta_entropy += np.sum(M_t2_r * np.log(M_t2_r.astype(float) / d_out_r_col / d_in[r]))\n",
    "    delta_entropy += np.sum(M_t2_s * np.log(M_t2_s.astype(float) / d_out_s_col / d_in[s]))\n",
    "    return delta_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Execute the best merge (agglomerative) moves to reduce a set number of blocks\n",
    "\n",
    "### Parameters\n",
    "    delta_entropy_for_each_block : ndarray (float)\n",
    "            the delta entropy for merging each block\n",
    "    best_merge_for_each_block : ndarray (int)\n",
    "            the best block to merge with for each block\n",
    "    b : ndarray (int)\n",
    "            array of block assignment for each node\n",
    "    B : int\n",
    "            total number of blocks in the current partition\n",
    "    B_to_merge : int\n",
    "            the number of blocks to merge\n",
    "\n",
    "### Returns\n",
    "    b : ndarray (int)\n",
    "            array of new block assignment for each node after the merge\n",
    "    B : int\n",
    "            total number of blocks after the merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def carry_out_best_merges(delta_entropy_for_each_block, best_merge_for_each_block, b, B, B_to_merge):\n",
    "    bestMerges = delta_entropy_for_each_block.argsort()\n",
    "    block_map = np.arange(B)\n",
    "    num_merge = 0\n",
    "    counter = 0\n",
    "    while num_merge < B_to_merge:\n",
    "        mergeFrom = bestMerges[counter]\n",
    "        mergeTo = block_map[best_merge_for_each_block[bestMerges[counter]]]\n",
    "        counter += 1\n",
    "        if mergeTo != mergeFrom:\n",
    "            block_map[np.where(block_map == mergeFrom)] = mergeTo\n",
    "            b[np.where(b == mergeFrom)] = mergeTo\n",
    "            num_merge += 1\n",
    "    remaining_blocks = np.unique(b)\n",
    "    mapping = -np.ones(B, dtype=int)\n",
    "    mapping[remaining_blocks] = np.arange(len(remaining_blocks))\n",
    "    b = mapping[b]\n",
    "    B -= B_to_merge\n",
    "    return b, B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Move the current node to the proposed block and update the edge counts\n",
    "\n",
    "### Parameters\n",
    "    b : ndarray (int)\n",
    "            current array of new block assignment for each node\n",
    "    ni : int\n",
    "            current node index\n",
    "    r : int\n",
    "            current block assignment for the node under consideration\n",
    "    s : int\n",
    "            proposed block assignment for the node under consideration\n",
    "    M : ndarray or sparse matrix (int), shape = (#blocks, #blocks)\n",
    "            edge count matrix between all the blocks.\n",
    "    M_r_row : ndarray or sparse matrix (int)\n",
    "            the current block row of the new edge count matrix under proposal\n",
    "    M_s_row : ndarray or sparse matrix (int)\n",
    "            the proposed block row of the new edge count matrix under proposal\n",
    "    M_r_col : ndarray or sparse matrix (int)\n",
    "            the current block col of the new edge count matrix under proposal\n",
    "    M_s_col : ndarray or sparse matrix (int)\n",
    "            the proposed block col of the new edge count matrix under proposal\n",
    "    d_out_new : ndarray (int)\n",
    "            the new out degree of each block under proposal\n",
    "    d_in_new : ndarray (int)\n",
    "            the new in degree of each block under proposal\n",
    "    d_new : ndarray (int)\n",
    "            the new total degree of each block under proposal\n",
    "    use_sparse : bool\n",
    "            whether the edge count matrix is stored as a sparse matrix\n",
    "### Returns\n",
    "    b : ndarray (int)\n",
    "            array of block assignment for each node after the move\n",
    "    M : ndarray or sparse matrix (int), shape = (#blocks, #blocks)\n",
    "            edge count matrix between all the blocks after the move\n",
    "    d_out_new : ndarray (int)\n",
    "            the out degree of each block after the move\n",
    "    d_in_new : ndarray (int)\n",
    "            the in degree of each block after the move\n",
    "    d_new : ndarray (int)\n",
    "            the total degree of each block after the move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def update_partition(b, ni, r, s, M, M_r_row, M_s_row, M_r_col, M_s_col, d_out_new, d_in_new, d_new, use_sparse):\n",
    "    b[ni] = s\n",
    "    M[r, :] = M_r_row\n",
    "    M[s, :] = M_s_row\n",
    "    if use_sparse:\n",
    "        M[:, r] = M_r_col\n",
    "        M[:, s] = M_s_col\n",
    "    else:\n",
    "        M[:, r] = M_r_col.reshape(M[:, r].shape)\n",
    "        M[:, s] = M_s_col.reshape(M[:, s].shape)\n",
    "    return b, M, d_out_new, d_in_new, d_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Compute the overall entropy for the current partition\n",
    "Compute the overall entropy, including the model entropy as well as the data entropy, on the current partition. The best partition with an optimal number of blocks will minimize this entropy.\n",
    "\n",
    "### Parameters\n",
    "    M : ndarray or sparse matrix (int), shape = (#blocks, #blocks)\n",
    "            edge count matrix between all the blocks.\n",
    "    d_out : ndarray (int)\n",
    "            the current out degrees of each block\n",
    "    d_in : ndarray (int)\n",
    "            the current in degrees of each block\n",
    "    B : int\n",
    "            the number of blocks in the partition\n",
    "    N : int\n",
    "            number of nodes in the graph\n",
    "    E : int\n",
    "            number of edges in the graph\n",
    "    use_sparse : bool\n",
    "            whether the edge count matrix is stored as a sparse matrix\n",
    "\n",
    "### Returns\n",
    "    S : float\n",
    "            the overall entropy of the current partition\n",
    "\n",
    "### Notes\n",
    "- $M$: current edge count matrix\n",
    "- $d_{t, \\rm out}$: current out degree of block $t$\n",
    "- $d_{t, \\rm in}$: current in degree of block $t$\n",
    "- $N$: number of nodes\n",
    "- $E$: number of edges\n",
    "- $B$: number of blocks\n",
    "- $C$: some constant invariant to the partition\n",
    "\n",
    "The overall entropy of the partition is computed as:\n",
    "\n",
    "$\\large S = E\\;h\\left(\\frac{B^2}{E}\\right) + N \\log(B) - \\sum_{t_1, t_2} {M_{t_1 t_2} \\log\\left(\\frac{M_{t_1 t_2}}{d_{t_1, \\rm out} d_{t_2, \\rm in}}\\right)} + C$\n",
    "\n",
    "where the function $h(x)=(1+x)\\log(1+x) - x\\log(x)$ and the sum runs over all entries $(t_1, t_2)$ in the edge count matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def compute_overall_entropy(M, d_out, d_in, B, N, E, use_sparse):\n",
    "    nonzeros = M.nonzero()  # all non-zero entries\n",
    "    edge_count_entries = M[nonzeros[0], nonzeros[1]]\n",
    "    if use_sparse:\n",
    "        edge_count_entries = edge_count_entries.toarray()\n",
    "\n",
    "    entries = edge_count_entries * np.log(edge_count_entries / (d_out[nonzeros[0]] * d_in[nonzeros[1]]).astype(float))\n",
    "    data_S = -np.sum(entries)\n",
    "    model_S_term = B**2 / float(E)\n",
    "    model_S = E * (1 + model_S_term) * np.log(1 + model_S_term) - model_S_term * np.log(model_S_term) + N*np.log(B)\n",
    "    S = model_S + data_S\n",
    "    return S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Prepare for partition on the next number of blocks\n",
    "This function checks to see whether the current partition has the optimal number of blocks. If not, the next number of blocks to try is determined and the intermediate variables prepared.\n",
    "\n",
    "### Parameters\n",
    "    S : float\n",
    "            the overall entropy of the current partition\n",
    "    b : ndarray (int)\n",
    "            current array of block assignment for each node\n",
    "    M : ndarray or sparse matrix (int), shape = (#blocks, #blocks)\n",
    "            edge count matrix between all the blocks.\n",
    "    d : ndarray (int)\n",
    "            the current total degree of each block\n",
    "    d_out : ndarray (int)\n",
    "            the current out degree of each block\n",
    "    d_in : ndarray (int)\n",
    "            the current in degree of each block\n",
    "    B : int\n",
    "            the number of blocks in the current partition\n",
    "    old_b : list of length 3\n",
    "            holds the best three partitions so far\n",
    "    old_M : list of length 3\n",
    "            holds the edge count matrices for the best three partitions so far\n",
    "    old_d : list of length 3\n",
    "            holds the block degrees for the best three partitions so far\n",
    "    old_d_out : list of length 3\n",
    "            holds the out block degrees for the best three partitions so far\n",
    "    old_d_in : list of length 3\n",
    "            holds the in block degrees for the best three partitions so far\n",
    "    old_S : list of length 3\n",
    "            holds the overall entropy for the best three partitions so far\n",
    "    old_B : list of length 3\n",
    "            holds the number of blocks for the best three partitions so far\n",
    "    B_rate : float\n",
    "            the ratio on the number of blocks to reduce before the golden ratio bracket is established\n",
    "\n",
    "    \n",
    "### Returns\n",
    "    b : ndarray (int)\n",
    "            starting array of block assignment on each node for the next number of blocks to try\n",
    "    M : ndarray or sparse matrix (int), shape = (#blocks, #blocks)\n",
    "            starting edge count matrix for the next number of blocks to try\n",
    "    d : ndarray (int)\n",
    "            the starting total degree of each block for the next number of blocks to try\n",
    "    d_out : ndarray (int)\n",
    "            the starting out degree of each block for the next number of blocks to try\n",
    "    d_in : ndarray (int)\n",
    "            the starting in degree of each block for the next number of blocks to try\n",
    "    B : int\n",
    "            the starting number of blocks before the next block merge\n",
    "    B_to_merge : int\n",
    "            number of blocks to merge next\n",
    "    old_b : list of length 3\n",
    "            holds the best three partitions including the current partition\n",
    "    old_M : list of length 3\n",
    "            holds the edge count matrices for the best three partitions including the current partition\n",
    "    old_d : list of length 3\n",
    "            holds the block degrees for the best three partitions including the current partition\n",
    "    old_d_out : list of length 3\n",
    "            holds the out block degrees for the best three partitions including the current partition\n",
    "    old_d_in : list of length 3\n",
    "            holds the in block degrees for the best three partitions including the current partition\n",
    "    old_S : list of length 3\n",
    "            holds the overall entropy for the best three partitions including the current partition\n",
    "    old_B : list of length 3\n",
    "            holds the number of blocks for the best three partitions including the current partition\n",
    "    optimal_B_found : bool\n",
    "            flag for whether the optimal block has been found\n",
    "\n",
    "\n",
    "### Notes\n",
    "The holders for the best three partitions so far and their statistics will be stored in the order of the number of blocks, starting from the highest to the lowest. The middle entry is always the best so far. The number of blocks is reduced by a fixed rate until the golden ratio bracket (three best partitions with the middle one being the best) is established. Once the golden ratio bracket is established, perform golden ratio search until the bracket is narrowed to consecutive number of blocks where the middle one is identified as the optimal number of blocks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def prepare_for_partition_on_next_num_blocks(S, b, M, d, d_out, d_in, B, old_b, old_M, old_d, old_d_out, old_d_in, old_S, old_B, B_rate):\n",
    "    optimal_B_found = False\n",
    "    B_to_merge = 0\n",
    "\n",
    "    # update the best three partitions so far and their statistics\n",
    "    if S <= old_S[1]:  # if the current partition is the best so far\n",
    "        # if the current number of blocks is smaller than the previous best number of blocks\n",
    "        old_index = 0 if old_B[1] > B else 2\n",
    "        old_b[old_index] = old_b[1]\n",
    "        old_M[old_index] = old_M[1]\n",
    "        old_d[old_index] = old_d[1]\n",
    "        old_d_out[old_index] = old_d_out[1]\n",
    "        old_d_in[old_index] = old_d_in[1]\n",
    "        old_S[old_index] = old_S[1]\n",
    "        old_B[old_index] = old_B[1]\n",
    "\n",
    "        index = 1\n",
    "    else:  # the current partition is not the best so far\n",
    "        # if the current number of blocks is smaller than the best number of blocks so far\n",
    "        index = 2 if old_B[1] > B else 0\n",
    "\n",
    "    old_b[index] = b\n",
    "    old_M[index] = M\n",
    "    old_d[index] = d\n",
    "    old_d_out[index] = d_out\n",
    "    old_d_in[index] = d_in\n",
    "    old_S[index] = S\n",
    "    old_B[index] = B\n",
    "\n",
    "    # find the next number of blocks to try using golden ratio bisection\n",
    "    if old_S[2] == np.Inf:  # if the three points in the golden ratio bracket has not yet been established\n",
    "        B_to_merge = int(B*B_rate)\n",
    "        b = old_b[1].copy()\n",
    "        M = old_M[1].copy()\n",
    "        d = old_d[1].copy()\n",
    "        d_out = old_d_out[1].copy()\n",
    "        d_in = old_d_in[1].copy()\n",
    "    else:  # golden ratio search bracket established\n",
    "        if old_B[0] - old_B[2] == 2:  # we have found the partition with the optimal number of blocks\n",
    "            optimal_B_found = True\n",
    "            B = old_B[1]\n",
    "            b = old_b[1]\n",
    "        else:  # not done yet, find the next number of block to try according to the golden ratio search\n",
    "            if (old_B[0]-old_B[1]) >= (old_B[1]-old_B[2]):  # the higher segment in the bracket is bigger\n",
    "                index = 0\n",
    "            else:  # the lower segment in the bracket is bigger\n",
    "                index = 1\n",
    "            next_B_to_try = old_B[index + 1] + np.round((old_B[index] - old_B[index + 1]) * 0.618).astype(int)\n",
    "            B_to_merge = old_B[index] - next_B_to_try\n",
    "            B = old_B[index]\n",
    "            b = old_b[index].copy()\n",
    "            M = old_M[index].copy()\n",
    "            d = old_d[index].copy()\n",
    "            d_out = old_d_out[index].copy()\n",
    "            d_in = old_d_in[index].copy()\n",
    "    return b, M, d, d_out, d_in, B, B_to_merge, old_b, old_M, old_d, old_d_out, old_d_in, old_S, old_B, optimal_B_found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Plot the graph with force directed layout and color/shape each node according to its block\n",
    "\n",
    "### Parameters\n",
    "    out_neighbors : list of ndarray; list length is N, the number of nodes\n",
    "                each element of the list is a ndarray of out neighbors, where the first column is the node indices\n",
    "                and the second column the corresponding edge weights\n",
    "    b : ndarray (int)\n",
    "                array of block assignment for each node\n",
    "    graph_object : graph tool object, optional\n",
    "                if a graph object already exists, use it to plot the graph\n",
    "### Returns\n",
    "    graph_object : graph tool object\n",
    "                the graph tool object containing the graph and the node position info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def plot_graph_with_partition(out_neighbors, b, graph_object=[]):\n",
    "    if use_graph_tool_options:  # nothing is done if the graph tool option is off\n",
    "        if len(out_neighbors) <= 5000:\n",
    "            if graph_object == []:\n",
    "                graph_object = gt.Graph()\n",
    "                graph_object.add_edge_list([(i,j) for i in range(len(out_neighbors)) for j in out_neighbors[i][:,0]])\n",
    "                graph_object.vp['pos'] = gt.sfdp_layout(graph_object)\n",
    "            block_membership = graph_object.new_vertex_property(\"int\")\n",
    "            vertex_shape = graph_object.new_vertex_property(\"int\")\n",
    "            block_membership.a = b\n",
    "            vertex_shape.a = np.mod(block_membership.a,10)\n",
    "            gt.graph_draw(graph_object, inline=True, output_size=(400,400), pos=graph_object.vp['pos'], vertex_shape=vertex_shape,\n",
    "                          vertex_fill_color=block_membership, edge_pen_width=0.07, edge_marker_size=1, vertex_size = 10)\n",
    "        else:\n",
    "            print('That\\'s a big graph!')\n",
    "    return graph_object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Evaluate the output partition against the truth partition and report the correctness metrics.\n",
    "\n",
    "### Parameters\n",
    "    true_b : ndarray (int)\n",
    "            array of truth block assignment for each node\n",
    "    alg_b : ndarray (int)\n",
    "            array of output block assignment for each node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def evaluate_partition(true_b, alg_b):\n",
    "    blocks_b1 = true_b\n",
    "    B_b1 = len(set(blocks_b1))\n",
    "\n",
    "    blocks_b2 = alg_b\n",
    "    B_b2 = max(blocks_b2) + 1\n",
    "\n",
    "    N = len(true_b)\n",
    "\n",
    "    print('\\nPartition Correctness Evaluation\\n')\n",
    "    print('Number of nodes: {}'.format(N))\n",
    "    print('Number of partitions in truth partition: {}'.format(B_b1))\n",
    "    print('Number of partitions in alg. partition: {}'.format(B_b2))\n",
    "\n",
    "    # populate the confusion matrix between the two partitions\n",
    "    contingency_table = np.zeros((B_b1, B_b2))\n",
    "    for i in range(0, N):\n",
    "        contingency_table[blocks_b1[i], blocks_b2[i]] += 1\n",
    "\n",
    "    # associate the labels between two partitions using linear assignment\n",
    "    assignment = Munkres()  # use the Hungarian algorithm / Kuhn-Munkres algorithm\n",
    "    if B_b1 > B_b2:  # transpose matrix for linear assignment (this implementation assumes #col >= #row)\n",
    "        contingency_table = contingency_table.transpose()\n",
    "    indexes = assignment.compute(-contingency_table)\n",
    "    total = 0\n",
    "    contingency_table_before_assignment = np.array(contingency_table)\n",
    "    for row, column in indexes:\n",
    "        contingency_table[:, row] = contingency_table_before_assignment[:, column]\n",
    "        total += contingency_table[row, row]\n",
    "    # fill in the un-associated columns\n",
    "    unassociated_col = set(range(contingency_table.shape[1])) - set(np.array(indexes)[:, 1])\n",
    "    counter = 0;\n",
    "    for column in unassociated_col:\n",
    "        contingency_table[:, contingency_table.shape[0] + counter] = contingency_table_before_assignment[:, column]\n",
    "        counter += 1\n",
    "    if B_b1 > B_b2:  # transpose back\n",
    "        contingency_table = contingency_table.transpose()\n",
    "    print('Contingency Table: \\n{}'.format(contingency_table))\n",
    "    joint_prob = contingency_table / sum(\n",
    "        sum(contingency_table))  # joint probability of the two partitions is just the normalized contingency table\n",
    "    accuracy = sum(joint_prob.diagonal())\n",
    "    print('Accuracy (with optimal partition matching): {}'.format(accuracy))\n",
    "    print('\\n')\n",
    "\n",
    "    # Compute pair-counting-based metrics\n",
    "    def nchoose2(a):\n",
    "        return misc.comb(a, 2)\n",
    "    num_pairs = nchoose2(N)\n",
    "    colsum = np.sum(contingency_table, axis=0)\n",
    "    rowsum = np.sum(contingency_table, axis=1)\n",
    "    # compute counts of agreements and disagreement (4 types) and the regular rand index\n",
    "    sum_table_squared = sum(sum(contingency_table ** 2))\n",
    "    sum_colsum_squared = sum(colsum ** 2)\n",
    "    sum_rowsum_squared = sum(rowsum ** 2)\n",
    "    count_in_each_b1 = np.sum(contingency_table, axis=1)\n",
    "    count_in_each_b2 = np.sum(contingency_table, axis=0)\n",
    "    num_same_in_b1 = sum(count_in_each_b1 * (count_in_each_b1 - 1)) / 2\n",
    "    num_same_in_b2 = sum(count_in_each_b2 * (count_in_each_b2 - 1)) / 2\n",
    "    num_agreement_same = 0.5 * sum(sum(contingency_table * (contingency_table - 1)));\n",
    "    num_agreement_diff = 0.5 * (N ** 2 + sum_table_squared - sum_colsum_squared - sum_rowsum_squared);\n",
    "    num_agreement = num_agreement_same + num_agreement_diff\n",
    "    rand_index = num_agreement / num_pairs\n",
    "\n",
    "    vectorized_nchoose2 = np.vectorize(nchoose2)\n",
    "    sum_table_choose_2 = sum(sum(vectorized_nchoose2(contingency_table)))\n",
    "    sum_colsum_choose_2 = sum(vectorized_nchoose2(colsum))\n",
    "    sum_rowsum_choose_2 = sum(vectorized_nchoose2(rowsum))\n",
    "    adjusted_rand_index = (sum_table_choose_2 - sum_rowsum_choose_2 * sum_colsum_choose_2 / num_pairs) / (\n",
    "    0.5 * (sum_rowsum_choose_2 + sum_colsum_choose_2) - sum_rowsum_choose_2 * sum_colsum_choose_2 / num_pairs)\n",
    "    print('Rand Index: {}'.format(rand_index))\n",
    "    print('Adjusted Rand Index: {}'.format(adjusted_rand_index))\n",
    "    print('Pairwise Recall: {}'.format(num_agreement_same / (num_same_in_b1)))\n",
    "    print('Pairwise Precision: {}'.format(num_agreement_same / (num_same_in_b2)))\n",
    "    print('\\n')\n",
    "\n",
    "    # compute the information theoretic metrics\n",
    "    marginal_prob_b2 = np.sum(joint_prob, 0)\n",
    "    marginal_prob_b1 = np.sum(joint_prob, 1)\n",
    "    conditional_prob_b2_b1 = joint_prob / marginal_prob_b1[:, None]\n",
    "    conditional_prob_b1_b2 = joint_prob / marginal_prob_b2[None, :]\n",
    "    # compute entropy of the non-partition2 and the partition2 version\n",
    "    idx = np.nonzero(marginal_prob_b2)\n",
    "    H_b2 = -np.sum(marginal_prob_b2[idx] * np.log(marginal_prob_b2[idx]))\n",
    "    idx = np.nonzero(marginal_prob_b1)\n",
    "    H_b1 = -np.sum(marginal_prob_b1[idx] * np.log(marginal_prob_b1[idx]))\n",
    "    # compute the conditional entropies\n",
    "    idx = np.nonzero(joint_prob)\n",
    "    H_b2_b1 = -np.sum(np.sum(joint_prob[idx] * np.log(conditional_prob_b2_b1[idx])))\n",
    "    H_b1_b2 = -np.sum(np.sum(joint_prob[idx] * np.log(conditional_prob_b1_b2[idx])))\n",
    "    # compute the mutual information (symmetric)\n",
    "    marginal_prod = np.dot(marginal_prob_b1[:, None], np.transpose(marginal_prob_b2[:, None]))\n",
    "    MI_b1_b2 = np.sum(np.sum(joint_prob[idx] * np.log(joint_prob[idx] / marginal_prod[idx])))\n",
    "    print('Entropy of truth partition: {}'.format(H_b1))\n",
    "    print('Entropy of alg. partition: {}'.format(H_b2))\n",
    "    print('Conditional entropy of truth partition given alg. partition: {}'.format(H_b1_b2))\n",
    "    print('Conditional entropy of alg. partition given truth partition: {}'.format(H_b2_b1))\n",
    "    print('Mututal informationion between truth partition and alg. partition: {}'.format(MI_b1_b2))\n",
    "    print('Fraction of missed information: {}'.format(H_b1_b2 / H_b1))\n",
    "    print('Fraction of erroneous information: {}'.format(H_b2_b1 / H_b2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Main routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    input_filename = '../../data/static/simulated_blockmodel_graph_500_nodes'\n",
    "    true_partition_available = True\n",
    "    visualize_graph = True # whether to plot the graph layout colored with intermediate partitions\n",
    "    verbose = True # whether to print updates of the partitioning\n",
    "\n",
    "    out_neighbors, in_neighbors, N, E, true_partition = load_graph(input_filename, true_partition_available)\n",
    "    if verbose:\n",
    "        print('Number of nodes: {}'.format(N))\n",
    "        print('Number of edges: {}'.format(E))\n",
    "\n",
    "    if use_timeit:\n",
    "        t0 = timeit.default_timer()\n",
    "\n",
    "    # initialize by putting each node in its own block (N blocks)\n",
    "    num_blocks = N\n",
    "    partition = np.array(range(num_blocks))\n",
    "\n",
    "    # partition update parameters\n",
    "    beta = 3 # exploitation versus exploration (higher value favors exploitation)\n",
    "    use_sparse_matrix = False # whether to represent the edge count matrix using sparse matrix \n",
    "                              # Scipy's sparse matrix is slow but this may be necessary for large graphs\n",
    "\n",
    "    # agglomerative partition update parameters\n",
    "    num_agg_proposals_per_block = 10 # number of proposals per block\n",
    "    num_block_reduction_rate = 0.5 # fraction of blocks to reduce until the golden ratio bracket is established\n",
    "\n",
    "    # nodal partition updates parameters\n",
    "    max_num_nodal_itr = 100 # maximum number of iterations\n",
    "    delta_entropy_threshold1 = 5e-4 # stop iterating when the change in entropy falls below this fraction of the overall entropy\n",
    "                                    # lowering this threshold results in more nodal update iterations and likely better performance, but longer runtime\n",
    "    delta_entropy_threshold2 = 1e-4 # threshold after the golden ratio bracket is established (typically lower to fine-tune to partition) \n",
    "    delta_entropy_moving_avg_window = 3 # width of the moving average window for the delta entropy convergence criterion\n",
    "\n",
    "    # initialize edge counts and block degrees\n",
    "    interblock_edge_count, block_degrees_out, block_degrees_in, block_degrees = initialize_edge_counts(out_neighbors, num_blocks, partition, use_sparse_matrix)\n",
    "\n",
    "    # initialize items before iterations to find the partition with the optimal number of blocks\n",
    "    optimal_num_blocks_found, old_partition, old_interblock_edge_count, old_block_degrees, old_block_degrees_out, old_block_degrees_in, old_overall_entropy, old_num_blocks, graph_object = initialize_partition_variables()\n",
    "    num_blocks_to_merge = int(num_blocks*num_block_reduction_rate)\n",
    "\n",
    "    # begin partitioning by finding the best partition with the optimal number of blocks\n",
    "    while not optimal_num_blocks_found:\n",
    "        # begin agglomerative partition updates (i.e. block merging)\n",
    "        if verbose:\n",
    "            print(\"\\nMerging down blocks from {} to {}\".format(num_blocks, num_blocks - num_blocks_to_merge))\n",
    "        best_merge_for_each_block = np.ones(num_blocks, dtype = int)*-1 # initialize to no merge\n",
    "        delta_entropy_for_each_block = np.ones(num_blocks)*np.Inf # initialize criterion\n",
    "        block_partition = range(num_blocks)\n",
    "        for current_block in range(num_blocks): # evalaute agglomerative updates for each block\n",
    "            for proposal_idx in range(num_agg_proposals_per_block):\n",
    "                # populate edges to neighboring blocks\n",
    "                if use_sparse_matrix:\n",
    "                    out_blocks = interblock_edge_count[current_block,:].nonzero()[1]\n",
    "                    out_blocks = np.hstack((out_blocks.reshape([len(out_blocks),1]), interblock_edge_count[current_block,out_blocks].toarray().transpose()))\n",
    "                else:\n",
    "                    out_blocks = interblock_edge_count[current_block,:].nonzero()\n",
    "                    out_blocks = np.hstack((np.array(out_blocks).transpose(), interblock_edge_count[current_block,out_blocks].transpose()))\n",
    "                if use_sparse_matrix:\n",
    "                    in_blocks = interblock_edge_count[:,current_block].nonzero()[0]\n",
    "                    in_blocks = np.hstack((in_blocks.reshape([len(in_blocks),1]), interblock_edge_count[in_blocks,current_block].toarray()))\n",
    "                else:\n",
    "                    in_blocks = interblock_edge_count[:,current_block].nonzero()\n",
    "                    in_blocks = np.hstack((np.array(in_blocks).transpose(), interblock_edge_count[in_blocks,current_block].transpose()))\n",
    "\n",
    "                # propose a new block to merge with\n",
    "                proposal, num_out_neighbor_edges, num_in_neighbor_edges, num_neighbor_edges = propose_new_partition(current_block, out_blocks, in_blocks, block_partition, interblock_edge_count, block_degrees, num_blocks, 1, use_sparse_matrix)\n",
    "\n",
    "                # compute the two new rows and columns of the interblock edge count matrix\n",
    "                new_interblock_edge_count_current_block_row, new_interblock_edge_count_new_block_row, new_interblock_edge_count_current_block_col, new_interblock_edge_count_new_block_col = \\\n",
    "                    compute_new_rows_cols_interblock_edge_count_matrix(interblock_edge_count, current_block, proposal, out_blocks[:,0], out_blocks[:,1], in_blocks[:,0], in_blocks[:,1], interblock_edge_count[current_block, current_block], 1, use_sparse_matrix)    \n",
    "\n",
    "                # compute new block degrees           \n",
    "                block_degrees_out_new, block_degrees_in_new, block_degrees_new = compute_new_block_degrees(current_block, proposal, block_degrees_out, block_degrees_in, block_degrees, num_out_neighbor_edges, num_in_neighbor_edges, num_neighbor_edges)\n",
    "\n",
    "                # compute change in entropy / posterior\n",
    "                delta_entropy = compute_delta_entropy(current_block, proposal, interblock_edge_count, new_interblock_edge_count_current_block_row, new_interblock_edge_count_new_block_row, new_interblock_edge_count_current_block_col, new_interblock_edge_count_new_block_col, block_degrees_out, block_degrees_in, block_degrees_out_new, block_degrees_in_new, use_sparse_matrix)\n",
    "                if delta_entropy < delta_entropy_for_each_block[current_block]: # a better block candidate was found\n",
    "                    best_merge_for_each_block[current_block] = proposal\n",
    "                    delta_entropy_for_each_block[current_block] = delta_entropy\n",
    "\n",
    "        # carry out the best merges\n",
    "        partition, num_blocks = carry_out_best_merges(delta_entropy_for_each_block, best_merge_for_each_block, partition, num_blocks, num_blocks_to_merge)\n",
    "\n",
    "        # re-initialize edge counts and block degrees\n",
    "        interblock_edge_count, block_degrees_out, block_degrees_in, block_degrees = initialize_edge_counts(out_neighbors, num_blocks, partition, use_sparse_matrix)\n",
    "\n",
    "        # perform nodal partition updates\n",
    "        if verbose:\n",
    "            print(\"Beginning nodal updates\")\n",
    "        total_num_nodal_moves = 0            \n",
    "        itr_delta_entropy = np.zeros(max_num_nodal_itr)\n",
    "\n",
    "        # compute the global entropy for MCMC convergence criterion\n",
    "        overall_entropy = compute_overall_entropy(interblock_edge_count, block_degrees_out, block_degrees_in, num_blocks, N, E, use_sparse_matrix)\n",
    "\n",
    "        for itr in range(max_num_nodal_itr):\n",
    "            num_nodal_moves = 0;\n",
    "            itr_delta_entropy[itr] = 0\n",
    "\n",
    "            for current_node in range(N):\n",
    "                current_block = partition[current_node] \n",
    "                # propose a new block for this node\n",
    "                proposal, num_out_neighbor_edges, num_in_neighbor_edges, num_neighbor_edges = propose_new_partition(current_block, out_neighbors[current_node], in_neighbors[current_node], partition, interblock_edge_count, block_degrees, num_blocks, 0, use_sparse_matrix)\n",
    "\n",
    "                # determine whether to accept or reject the proposal\n",
    "                if (proposal != current_block):\n",
    "                    # compute block counts of in and out neighbors\n",
    "                    blocks_out, inverse_idx_out = np.unique(partition[out_neighbors[current_node][:,0]], return_inverse = True)\n",
    "                    count_out = np.bincount(inverse_idx_out, weights=out_neighbors[current_node][:,1]).astype(int)\n",
    "                    blocks_in, inverse_idx_in = np.unique(partition[in_neighbors[current_node][:,0]], return_inverse = True)\n",
    "                    count_in = np.bincount(inverse_idx_in, weights=in_neighbors[current_node][:,1]).astype(int)\n",
    "\n",
    "                    # compute the two new rows and columns of the interblock edge count matrix\n",
    "                    self_edge_weight = np.sum(out_neighbors[current_node][np.where(out_neighbors[current_node][:,0]==current_node),1]) # check if this node has a self edge\n",
    "                    new_interblock_edge_count_current_block_row, new_interblock_edge_count_new_block_row, new_interblock_edge_count_current_block_col, new_interblock_edge_count_new_block_col = \\\n",
    "                        compute_new_rows_cols_interblock_edge_count_matrix(interblock_edge_count, current_block, proposal, blocks_out, count_out, blocks_in, count_in, self_edge_weight, 0, use_sparse_matrix)\n",
    "\n",
    "                    # compute new block degrees           \n",
    "                    block_degrees_out_new, block_degrees_in_new, block_degrees_new = compute_new_block_degrees(current_block, proposal, block_degrees_out, block_degrees_in, block_degrees, num_out_neighbor_edges, num_in_neighbor_edges, num_neighbor_edges)\n",
    "\n",
    "                    # compute the Hastings correction\n",
    "                    Hastings_correction = compute_Hastings_correction(blocks_out, count_out, blocks_in, count_in, proposal, interblock_edge_count, new_interblock_edge_count_current_block_row, new_interblock_edge_count_current_block_col, num_blocks, block_degrees, block_degrees_new, use_sparse_matrix)\n",
    "\n",
    "                    # compute change in entropy / posterior\n",
    "                    delta_entropy = compute_delta_entropy(current_block, proposal, interblock_edge_count, new_interblock_edge_count_current_block_row, new_interblock_edge_count_new_block_row, new_interblock_edge_count_current_block_col, new_interblock_edge_count_new_block_col, block_degrees_out, block_degrees_in, block_degrees_out_new, block_degrees_in_new, use_sparse_matrix)\n",
    "\n",
    "                    # compute probability of acceptance\n",
    "                    p_accept = np.min([np.exp(-beta*delta_entropy)*Hastings_correction, 1])\n",
    "\n",
    "                    # if accept the proposal, update the partition, inter_block_edge_count, and block degrees\n",
    "                    if (np.random.uniform() <= p_accept):\n",
    "                        total_num_nodal_moves += 1\n",
    "                        num_nodal_moves += 1\n",
    "                        itr_delta_entropy[itr] += delta_entropy\n",
    "                        partition, interblock_edge_count, block_degrees_out, block_degrees_in, block_degrees = update_partition(partition, current_node, current_block, proposal, interblock_edge_count, new_interblock_edge_count_current_block_row, new_interblock_edge_count_new_block_row, new_interblock_edge_count_current_block_col, new_interblock_edge_count_new_block_col, block_degrees_out_new, block_degrees_in_new, block_degrees_new, use_sparse_matrix)\n",
    "            if verbose:\n",
    "                print(\"Itr: {}, number of nodal moves: {}, delta S: {:0.5f}\".format(itr, num_nodal_moves, itr_delta_entropy[itr]/float(overall_entropy)))\n",
    "            if itr>=(delta_entropy_moving_avg_window-1): # exit MCMC if the recent change in entropy falls below a small fraction of the overall entropy\n",
    "                if not(np.all(np.isfinite(old_overall_entropy))): # golden ratio bracket not yet established \n",
    "                    if (-np.mean(itr_delta_entropy[(itr-delta_entropy_moving_avg_window+1):itr]) < (delta_entropy_threshold1*overall_entropy)):\n",
    "                        break\n",
    "                else: # golden ratio bracket is established. Fine-tuning partition.\n",
    "                    if (-np.mean(itr_delta_entropy[(itr-delta_entropy_moving_avg_window+1):itr]) < (delta_entropy_threshold2*overall_entropy)):\n",
    "                        break\n",
    "\n",
    "        # compute the global entropy for determining the optimal number of blocks\n",
    "        overall_entropy = compute_overall_entropy(interblock_edge_count, block_degrees_out, block_degrees_in, num_blocks, N, E, use_sparse_matrix)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Total number of nodal moves: {}, overall_entropy: {:0.2f}\".format(total_num_nodal_moves, overall_entropy))\n",
    "        if visualize_graph:\n",
    "            graph_object = plot_graph_with_partition(out_neighbors, partition, graph_object)\n",
    "\n",
    "        # check whether the partition with optimal number of block has been found; if not, determine and prepare for the next number of blocks to try\n",
    "        partition, interblock_edge_count, block_degrees, block_degrees_out, block_degrees_in, num_blocks, num_blocks_to_merge, old_partition, old_interblock_edge_count, old_block_degrees, old_block_degrees_out, old_block_degrees_in, old_overall_entropy, old_num_blocks, optimal_num_blocks_found = \\\n",
    "            prepare_for_partition_on_next_num_blocks(overall_entropy, partition, interblock_edge_count, block_degrees, block_degrees_out, block_degrees_in, num_blocks, old_partition, old_interblock_edge_count, old_block_degrees, old_block_degrees_out, old_block_degrees_in, old_overall_entropy, old_num_blocks, num_block_reduction_rate)\n",
    "\n",
    "        if verbose:\n",
    "            print('Overall entropy: {}'.format(old_overall_entropy))\n",
    "            print('Number of blocks: {}'.format(old_num_blocks))\n",
    "            if optimal_num_blocks_found:\n",
    "                print('\\nOptimal partition found with {} blocks'.format(num_blocks))\n",
    "    if use_timeit:\n",
    "        t1 = timeit.default_timer()\n",
    "        print('\\nGraph partition took {} seconds'.format(t1-t0))\n",
    "\n",
    "    # evaluate output partition against the true partition\n",
    "    evaluate_partition(true_partition, partition)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
